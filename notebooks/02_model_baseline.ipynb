{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Calibrated Logistic Baseline\n",
        "\n",
        "One-vs-rest logistic regression with calibration. Train on older seasons, validate on 2024/25, test on 2025/26 to date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
        "from sklearn.metrics import log_loss, brier_score_loss\n",
        "\n",
        "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
        "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"market_epl.parquet\"\n",
        "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "pd.options.display.float_format = \"{:.4f}\".format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load processed market dataset\n",
        "Run `01_backtest_naive.ipynb` first to build `data/processed/market_epl.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RESULT_COL = \"FTR\"\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "def load_market_dataset(path: Path = DATA_PATH) -> pd.DataFrame:\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(\"Run 01_backtest_naive.ipynb to build data/processed/market_epl.parquet.\")\n",
        "    return pd.read_parquet(path)\n",
        "\n",
        "\n",
        "def season_start_year(season: str) -> Optional[int]:\n",
        "    if isinstance(season, str) and \"-\" in season:\n",
        "        try:\n",
        "            return int(season.split(\"-\")[0])\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "data = load_market_dataset()\n",
        "data[\"season_start\"] = data.get(\"season\", pd.Series(dtype=str)).apply(season_start_year)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature set and splits\n",
        "Add pre-match features (form, rolling goal/shots differentials, league position deltas) to `FEATURE_COLS` once engineered. Only use information available before kickoff to avoid leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FEATURE_COLS = [\n",
        "    \"odds_home\",\n",
        "    \"odds_draw\",\n",
        "    \"odds_away\",\n",
        "    # TODO: add pre-match stats (rolling goal diff, shots, form, league position deltas)\n",
        "]\n",
        "\n",
        "TRAIN_START = 2016\n",
        "TRAIN_END = 2023\n",
        "VALID_SEASON = 2024\n",
        "TEST_SEASON = 2025\n",
        "\n",
        "model_df = data.dropna(subset=FEATURE_COLS + [RESULT_COL]).copy()\n",
        "model_df = model_df[model_df[\"season_start\"].notna()]\n",
        "\n",
        "train_df = model_df[(model_df[\"season_start\"] >= TRAIN_START) & (model_df[\"season_start\"] <= TRAIN_END)]\n",
        "valid_df = model_df[model_df[\"season_start\"] == VALID_SEASON]\n",
        "test_df = model_df[model_df[\"season_start\"] == TEST_SEASON]\n",
        "\n",
        "print({\n",
        "    \"train_rows\": len(train_df),\n",
        "    \"valid_rows\": len(valid_df),\n",
        "    \"test_rows\": len(test_df),\n",
        "})\n",
        "\n",
        "X_train = train_df[FEATURE_COLS]\n",
        "y_train = train_df[RESULT_COL]\n",
        "X_valid = valid_df[FEATURE_COLS]\n",
        "y_valid = valid_df[RESULT_COL]\n",
        "X_test = test_df[FEATURE_COLS]\n",
        "y_test = test_df[RESULT_COL]\n",
        "\n",
        "if X_train.empty:\n",
        "    raise ValueError(\"Training set is empty. Check season labels and FEATURE_COLS.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit one-vs-rest logistic regression with calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = LogisticRegression(\n",
        "    multi_class=\"ovr\",\n",
        "    C=1.0,\n",
        "    penalty=\"l2\",\n",
        "    max_iter=500,\n",
        ")\n",
        "\n",
        "calibrated_model = CalibratedClassifierCV(\n",
        "    base_estimator=base_model,\n",
        "    method=\"isotonic\",\n",
        "    cv=5,\n",
        ")\n",
        "\n",
        "calibrated_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate log loss, Brier score, and calibration curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_split(name: str, X: pd.DataFrame, y: pd.Series):\n",
        "    if X.empty:\n",
        "        print(f\"[warn] {name} split is empty.\")\n",
        "        return None\n",
        "    probs = calibrated_model.predict_proba(X)\n",
        "    classes = calibrated_model.classes_\n",
        "    prob_df = pd.DataFrame(probs, columns=[f\"model_p{c}\" for c in classes])\n",
        "\n",
        "    ll = log_loss(y, probs, labels=classes)\n",
        "    brier_components = []\n",
        "    for cls in classes:\n",
        "        brier_components.append(brier_score_loss((y == cls).astype(int), prob_df[f\"model_p{cls}\"]))\n",
        "    brier = float(np.mean(brier_components))\n",
        "\n",
        "    print(f\"{name} log loss: {ll:.4f} | Brier (macro): {brier:.4f}\")\n",
        "    return prob_df, classes, ll, brier\n",
        "\n",
        "\n",
        "eval_valid = evaluate_split(\"valid\", X_valid, y_valid)\n",
        "eval_test = evaluate_split(\"test\", X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if eval_valid:\n",
        "    prob_df, classes, _, _ = eval_valid\n",
        "    fig, axes = plt.subplots(1, len(classes), figsize=(5 * len(classes), 4), sharey=True)\n",
        "    if len(classes) == 1:\n",
        "        axes = [axes]\n",
        "    for ax, cls in zip(axes, classes):\n",
        "        true_binary = (y_valid == cls).astype(int)\n",
        "        prob_pos = prob_df[f\"model_p{cls}\"]\n",
        "        frac_pos, mean_pred = calibration_curve(true_binary, prob_pos, n_bins=10, strategy=\"quantile\")\n",
        "        ax.plot(mean_pred, frac_pos, marker=\"o\", label=\"Observed\")\n",
        "        ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Perfect\")\n",
        "        ax.set_title(f\"Calibration for {cls} (valid)\")\n",
        "        ax.set_xlabel(\"Predicted prob\")\n",
        "        ax.set_ylabel(\"Observed freq\")\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save test predictions for downstream comparisons\n",
        "Stores market and model probabilities for each outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds_test = test_df.reset_index(drop=True).copy()\n",
        "if eval_test:\n",
        "    prob_df, classes, _, _ = eval_test\n",
        "    for cls in classes:\n",
        "        preds_test[f\"model_p{cls}\"] = prob_df[f\"model_p{cls}\"]\n",
        "\n",
        "preds_path = PROJECT_ROOT / \"reports\" / \"predictions_baseline.csv\"\n",
        "preds_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "preds_test.to_csv(preds_path, index=False)\n",
        "preds_path"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}